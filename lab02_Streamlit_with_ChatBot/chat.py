import streamlit as st # Streamlit ë¼ì´ë¸ŒëŸ¬ë¦¬

from langchain_core.output_parsers import StrOutputParser # ë¬¸ìì—´ ì¶œë ¥ íŒŒì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_core.prompts import ChatPromptTemplate     # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain.chains import RetrievalQA                  # RetrievalQA ì²´ì¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain import hub                                 # LangChain Hub ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import ChatOpenAI                   # OpenAIì˜ GPTëª¨ë¸ë¡œ ëŒ€í™”í˜•AI ë¼ì´ë¸ŒëŸ¬ë¦¬
from dotenv import load_dotenv                            # í™˜ê²½ë³€ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_openai import OpenAIEmbeddings             # ì„ë² ë”© ëª¨ë¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
from langchain_pinecone import PineconeVectorStore        # Pinecone ë²¡í„°DB ë¼ì´ë¸ŒëŸ¬ë¦¬

load_dotenv()  # í™˜ê²½ë³€ìˆ˜ ë¶ˆëŸ¬ì˜¤ê¸°

st.set_page_config(page_title='ì†Œë“ì„¸ ì±—ë´‡', page_icon="ğŸ˜¶â€ğŸŒ«ï¸")

st.title("ğŸ˜’ì†Œë“ì„¸ ì±—ë´‡")
st.caption("ì†Œë“ì„¸ì— ê´€ë ¨ëœ ëª¨ë“ ê²ƒì„ ë‹µí•´ë“œë¦½ë‹ˆë‹¤!")

# st.session_state ëŠ” Streamlit ì˜ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬ ê°ì²´, Key-Value êµ¬ì¡°.
if 'message_list' not in st.session_state:
    st.session_state.message_list = []


# ì„¸ì…˜ì—ì„œ ì €ì¥ëœ ë©”ì‹œì§€ í™”ë©´ì— ì¶œë ¥.
for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# LLM ì§ˆì˜ í•¨ìˆ˜
def get_ai_message(user_message):
    embeddings = OpenAIEmbeddings(model='text-embedding-3-large')  # ê¸°ë³¸ëª¨ë¸ì€ 002ì¸ ì˜ˆì „ëª¨ë¸ì´ë‹¤. ì‹ ê·œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì¶”ê°€
    index_name = 'tax-markdown-index'
    database = PineconeVectorStore.from_existing_index(embedding=embeddings, index_name=index_name)
    llm = ChatOpenAI(model='gpt-4o')
    prompt = hub.pull("rlm/rag-prompt")
    retriever = database.as_retriever(search_kwargs={'k': 4})
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever=retriever,
        chain_type_kwargs={"prompt": prompt}
    )
    dictionary = ['ì‚¬ëŒì„ ë‚˜íƒ€ë‚´ëŠ” í‘œí˜„ -> ê±°ì£¼ì']

    prompt = ChatPromptTemplate.from_template(
    f"""
        ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ê³ , ìš°ë¦¬ì˜ ì‚¬ì „ì„ ì°¸ê³ í•´ì„œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•´ì£¼ì„¸ìš”.
        ë§Œì•½ ë³€ê²½í•  í•„ìš”ê°€ ì—†ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´, ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³€ê²½í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.
        ì‚¬ì „: {dictionary}

        ì§ˆë¬¸: {{question}}
    """)

    dictionary_chain = prompt | llm | StrOutputParser()  # StrOutputParser() ëŠ” ê²°ê³¼ë¥¼ Stringìœ¼ë¡œ
    tax_chain = {"query": dictionary_chain} | qa_chain  # íŒŒì‹±
    ai_message = tax_chain.invoke({"question": user_message})
    return ai_message["result"]

#:=(ìœŒëŸ¬ìŠ¤)ì—°ì‚°ìëŠ” Python 3.8ì— ë„ì…ëœ ê¸°ëŠ¥ì´ë‹¤.
#êµ¬ì¡° -> variable := expression
#í•´ì„ -> expression ì˜ ê°’ì„ í‰ê°€í•œ ë’¤, ê·¸ ê²°ê³¼ë¥¼ variableì— ì €ì¥.
if user_question := st.chat_input(placeholder='ì†Œë“ì„¸ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!'):
    # ì‚¬ìš©ì
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role":"user", "content":user_question})

    with st.spinner("ë‹µë³€ì„ ìƒì„±ì¤‘ì…ë‹ˆë‹¤..."):
        # ì±—ë´‡(AI)
        ai_mesesage = get_ai_message(user_question)
        with st.chat_message("ai"):
            st.write(ai_mesesage)
        st.session_state.message_list.append({"role":"ai", "content":ai_mesesage})


